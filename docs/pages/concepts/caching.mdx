import { Callout } from 'nextra-theme-docs'

# Caching


Caching is a technique that allows you to store the results of expensive operations and reuse them later. This is useful when you have a function that takes a long time to run, but you need to call it multiple times with the same arguments. Instead of running the function every time you need the result, you can cache the result the first time the function is run and then retrieve the cached result every subsequent time the function is run.

## Caching in LLM
This principle of caching still applies to LLM models. Although it is a similarity search cache rather than hash index cache, it is the sample princple. Caching means more money saved, and response times that are orders of magnitude faster.

Examples include redis, memcached, or GPTCache (https://github.com/zilliztech/GPTCache).
